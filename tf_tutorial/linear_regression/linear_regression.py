# linear regression
# Author: Nishith Khandwala (nishith@stanford.edu)
# Adapted from https://github.com/hans/ipython-notebooks/

from __future__ import absolute_import
from __future__ import division
from __future__ import print_function

import numpy as np
import tensorflow as tf
import matplotlib

matplotlib.use('TKAgg')
from matplotlib import pyplot as plt

'''
Good old linear regression: find the best linear fit to our data
'''


def generate_dataset():
    # data is generated by y = 2x + e
    # where 'e' is sampled from a normal distribution
    x_batch = np.linspace(-1, 1, 101)
    y_batch = 2 * x_batch + np.random.randn(*x_batch.shape) * 0.3
    return x_batch, y_batch


def linear_regression():
    # (1) create placeholders and variables
    x = tf.placeholder(tf.float32, shape=(None,), name='x')
    y = tf.placeholder(tf.float32, shape=(None,), name='y')

    with tf.variable_scope('lreg') as scope:
        w = tf.Variable(np.random.normal(), name='W')
        # (2) forward pass and loss
        y_pred = tf.multiply(w, x)

        loss = tf.reduce_mean(tf.square(y_pred - y))
    return x, y, y_pred, loss


def run():
    # it seems we actually don't use any batches
    x_batch, y_batch = generate_dataset()

    x, y, y_pred, loss = linear_regression()
    # why do we use minimize(loss) ?
    optimizer = tf.train.GradientDescentOptimizer(0.1).minimize(loss)
    init = tf.global_variables_initializer()

    with tf.Session() as session:
        session.run(init)

        feed_dict = {x: x_batch, y: y_batch}
        for _ in range(30):
            # why do we fetch both values, not only optimizer?
            loss_val, _ = session.run([loss, optimizer], feed_dict)
            print("loss:", loss_val)

        y_pred_batch = session.run(y_pred, {x: x_batch})

    plt.figure(1)
    plt.scatter(x_batch, y_batch)
    plt.plot(x_batch, y_pred_batch)
    plt.savefig('plot.png')


if __name__ == '__main__':
    run()
