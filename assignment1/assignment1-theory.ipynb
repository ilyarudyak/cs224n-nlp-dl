{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## question 1 (a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prove that softmax is invariant to constant offsets in the input, that is, for any input vector $x$ and any constant $c$: $softmax(x) = softmax(x + c)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take $i^{th}$ component of the right term: \n",
    "\n",
    "$$\\frac{e^{x_i+c}}{\\sum_j{e^{x_j+c}}} = \\frac{e^c e^{x_i}}{e^c \\sum_j{e^{x_j}}} = \\frac{e^{x_i}}{\\sum_j{e^{x_j}}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So this is rather easy. Let's also mention here the numerically stable version of the `softmax` function - see `cs231n` [notes](http://cs231n.github.io/linear-classify/#softmax)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = np.array([123, 456, 789]) # example with 3 classes and each having large scores\n",
    "# p = np.exp(f) / np.sum(np.exp(f)) # Bad: Numeric problem, potential blowup\n",
    "\n",
    "# instead: first shift the values of f so that the highest number is 0:\n",
    "f -= np.max(f) # f becomes [-666, -333, 0]\n",
    "p = np.exp(f) / np.sum(np.exp(f)) # safe to do, gives the correct answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## question 2 (b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Derive the gradient with regard to the inputs of a softmax function when cross entropy loss is used for evaluation, i.e., find the gradients with respect to the softmax input vector $x$, when the prediction is made by $\\hat{y} = softmax(x)$. Remember the cross entropy function is:\n",
    "\n",
    "$$CE(y, \\hat{y}) = -\\sum_i{y_i log(\\hat{y}_i)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here $y$ is a one-hot encoded vector. So we may write this as (where $s$ is the correct class):\n",
    "\n",
    "$$CE(y, \\hat{y}) = -log(\\hat{y}_s) = -log(\\frac{e^{x_s}}{\\sum_j{e^{x_j}}})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's prove that:\n",
    "\n",
    "$$\\frac{\\partial CE(y, \\hat{y})}{\\partial x} = \\hat{y} - y$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Case 1: $i \\neq s$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\frac{\\partial CE(y, \\hat{y})}{\\partial x_i} = -\\frac{\\sum_j{e^{x_j}}}{e^{x_s}} (-\\frac{e^{x_s}}{(\\sum_j{e^{x_j}})^2})e^{x_i} = \\frac{e^{x_i}}{\\sum_j{e^{x_j}}} = \\hat{y}_i$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Case 2: $i = s$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\frac{\\partial CE(y, \\hat{y})}{\\partial x_i} = -\\frac{\\sum_j{e^{x_j}}}{e^{x_s}} (\\frac{e^{x_s} \\sum_j{e^{x_j}} - (e^{x_s})^2}{(\\sum_j{e^{x_j}})^2}) = -1 + \\frac{e^{x_s}}{\\sum_j{e^{x_j}}} = \\hat{y}_s - 1$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we just have to recall that $y$ is a one-hot encoded vector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## question 2 (c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Derive the gradients with respect to the inputs x to an one-hidden-layer neural network (that is,find $\\frac{\\partial J}{\\partial x}$ where $J = CE(y, \\hat{y})$ is the cost function for the neural network).The neural network employs `sigmoid` activation function for the hidden layer, and `softmax` for the output layer. Assume the one-hot label vector is $y$, and cross entropy cost is used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that the forward propagation is as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$h = sigmoid(x W_1 + b_1) = sigmoid(z_1)$$ $$\\hat{y} = softmax(h W_2 + b_2) = softmax(z_2)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### step 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's prove that: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\delta_1 = \\frac{\\partial CE}{\\partial z_2} = \\hat{y} - y $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This directly follows from the previous result - this is a backprop through the `softmax`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### step 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now prove that:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\delta_2 = \\frac{\\partial CE}{\\partial h} = \\delta_1 \\frac{\\partial z_2}{\\partial h} = \\delta_1 W^T_2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's prove this without using jacobians."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\frac{\\partial CE}{\\partial h_i} = \\sum_j{\\frac{\\partial CE}{\\partial z_{2j}} \\frac{\\partial z_{2j}}{\\partial h_i}} = \\sum_j{ \\delta_{1j} \\frac{\\partial z_{2j}}{\\partial h_i} } $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$z_2 = h W_2 = [h (W_2)_{*1} ... h (W_2)_{*j} ...]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\frac{\\partial z_{2j}}{\\partial h_i} = (W_2)_{ij}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\frac{\\partial CE}{\\partial h_i} = \\sum_j{ \\delta_{1j} (W_2)_{ij} } = \\delta_{1} (W^T_2)_{*i}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### step 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now prove that:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\delta_3 = \\frac{\\partial CE}{\\partial z_1} = \\delta_2 \\frac{\\partial h}{\\partial z_1} = \\delta_2 \\circ \\sigma^\\prime(z_1)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's prove this without using jacobians."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\frac{\\partial CE}{\\partial z_{1i}} = \\sum_j{ \\frac{\\partial CE}{\\partial z_{2j}} \\frac{\\partial z_{2j}}{\\partial z_{1i}} } = \\sum_j{ \\frac{\\partial CE}{\\partial z_{2j}} \\frac{\\partial z_{2j}}{\\partial h_i} \\sigma^\\prime(z_{1i}) } = \\delta_{2i} \\sigma^\\prime(z_{1i})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This follows from the following equality where the key is the second equality. It follows from the fact that $h_i$ depends only on $z_{1i}$: $h = [\\sigma(z_{11}) ... \\sigma(z_{1i}) ...]$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\frac{\\partial z_{2j}}{\\partial z_{1i}} = \\sum_k{ \\frac{\\partial z_{2j}}{\\partial h_k} \\frac{\\partial h_k}{\\partial z_{1i}} } = \\frac{\\partial z_{2j}}{\\partial h_i} \\frac{\\partial h_i}{\\partial z_{1i}} = \\frac{\\partial z_{2j}}{\\partial h_i} \\sigma^\\prime(z_{1i})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### step 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally let's prove that:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\frac{\\partial CE}{\\partial x} = \\delta_3 \\frac{\\partial z_1}{\\partial x} = \\delta_3 W^T_1$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's prove this without using jacobians. We use here results from the step 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\frac{\\partial CE}{\\partial x_i} = \\sum_j{ \\frac{\\partial CE}{\\partial z_{1j}} \\frac{\\partial z_{1j}}{\\partial x_i} } = \\sum_j{ \\delta_{3j} (W_1)_{ij} } = \\delta_{3} (W^T_1)_{*i}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
